<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: tdd | dave^2 = -1]]></title>
  <link href="http://davesquared.net/categories/tdd/atom.xml" rel="self"/>
  <link href="http://davesquared.net/"/>
  <updated>2022-10-15T16:55:10+11:00</updated>
  <id>http://davesquared.net/</id>
  <author>
    <name><![CDATA[David Tchepak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Don't mock types you don't own]]></title>
    <link href="http://davesquared.net/2011/04/dont-mock-types-you-dont-own.html"/>
    <updated>2011-04-28T23:57:00+10:00</updated>
    <id>http://davesquared.net/2011/04/dont-mock-types-you-dont-own</id>
    <content type="html"><![CDATA[<p>I've found applying the guideline* of <a href="http://stevef.truemesh.com/archives/000194.html">not mocking types you don't own</a> to have helped my designs a fair bit recently. I've <a href="http://davesquared.net/2010/06/tdd-not-utdd.html">mentioned it before</a>, but I wanted to take a closer look at the topic in an attempt to improve my own understanding. And in that spirit of improving my understanding, please feel free to rip this to shreds. :)</p>




<div class="note"><b>*</b> Guideline, not a <a href="http://davesquared.net/2011/01/rules.html">rule</a>. :)</div>




<h2>But I don't use mocks! I use stubs!</h2>




<p>Let's start by clarifying that by "mocking" I'm referring to the <a href="http://martinfowler.com/articles/mocksArentStubs.html">mocks and stubs</a> we use as test doubles; the kind we tend to get from our mocking/substitution/fake/isolation framework of choice. By "types we don't own" we're talking about any type that isn't defined and tested as part of the current build (or solution, to use Visual Studio parlance). This includes anything from types in a base class library to APIs for accessing another system or service.</p>




<h2>When mocking makes sense</h2>




<p>To me the sweet spot for mocking is when we're using TDD to drive out the design of a collaborator that doesn't exist yet, or one that requires modification.</p>




<p>In these cases we're defining how the type will act, and we have the ability to adjust this behaviour as required by the type's consumers by altering these tests. At this level our design is really quite fluid; we can shuffle responsibilities between types, and create new or collapse existing types as we see fit. We are working within our own abstractions, optimised (hopefully) for making our code easy to work with.</p>




<p>A more general form of this case is when we want to isolate a class under test from its dependencies. This allows us to configure explicit behaviour for our test doubles so we don't need to test a whole host of input combinations; we can assume the collaborators' behaviours and just worry about the class under test. It also can shield us from long running tests by replacing slow collaborators (such as those that make database or web service calls), or from difficult test setups due to collaborators that need to run in an explicitly configured environment (such as those that make database or web service calls), or from unreliable tests due to collaborators with variable behaviour (such as those that make database or web service calls ;)).</p>




<p>However this more general case has landed me in trouble more than a few times when I've tried mocking (and stubbing) the interactions with types I don't own.</p>




<h2>Tests that do little</h2>




<p>When we are writing a test as part of TDD we are primarily specifying how our type will behave, and as a result defining the behaviour of the type's collaborators. The interactions between these types are tested as well, but the next step we'll do is drive down into the collaborating type and make sure it will work as required. As we have full control over the types at each end of the exchange, we can be reasonably confident our real code is going to do what we expect. After all, we wrote and tested that code in accordance with how our tests told us it was going to behave.</p>




<p>For types we don't own we don't have this level of transparency. They already have a fixed design and behaviour specified elsewhere. By testing interactions with a mocked version of this type, we really are not using our test to check for the correct behaviour, nor to drive out a collaborator's design. All our test is doing is reiterating our guess as to how the other type works. Sure, it's better than no test, but not necessarily by much. Just because we have checked we successfully called <code>database.Save(data)</code> doesn't mean that we didn't need to call <code>database.OpenConnection()</code> first.</p>




<p>Even when we know an external library really well, and so completely understand the interactions required, mocking that library can still give us misleading tests. If we update the library version and there has been a subtle change to the required interactions, our tests can still pass but our code will fail. Just because we have always received events in a particular order from a library doesn't mean we can assume that will always be the case, particularly if it is not documented behaviour and/or it was just a side-effect of the implementation used in previous versions.</p>




<p>These integration issues can also be a problem with types we do own, but I've found that because we're driving out each step in an end-to-end behaviour, we get added transparency and more focussed abstractions that tend to make this much less common than when dealing with external types.</p>




<h2>Tests that do too much</h2>




<p>Another risk we run in specifying all the required interactions with types we don't own is having <a href="http://xunitpatterns.com/Fragile%20Test.html#Overcoupled%20Test">overspecified tests</a>. These tests quickly become a pain point; they are hard to change because they are so coupled to the specific implementation and pattern of interactions with a type, and refactoring which doesn't actually change the behaviour can result in tests failing.</p>




<p>This can also mean large, convoluted setups as we attempt to mimic the way the collaborating types work, which can make our tests difficult to read, understand and maintain. In general, once we find ourselves setting up complex interactions and behaviour using mocking framework constructs, chances are we're doing it wrong.</p>




<p>This can be mitigated to some extent by the way we encapsulate the library and the setup needed for our test fixtures, but eventually we're going to have to pay for the impedance mismatch between our abstractions and the abstractions used for the type we don't own. Which segues neatly to our next point...</p>




<h2>Corrupted abstractions</h2>




<p>Depending on the complexity of the interactions with types we don't own, the abstractions used for those types can leak into and influence our own abstractions. We may add abstractions just to give us enough visibility into interactions with these types to ensure we have specified all the calls we expect, rather than because they make sense to the problem we are trying to solve. This could mean a whole lot of wrappers over external types, or new factories to create specific objects required by the interactions.</p>




<p>This isn't necessarily bad, but it does mean we're no longer writing the code that best solves or abstracts away our current problem, and also can leave us more tightly coupled than we'd like to the library that owns the type. Because we're not defining the behaviour in our tests, we end up coupling our design to the implementation of the external types instead.</p>




<p>Writing and maintaining these additional abstractions can also be quite expensive, especially considering the questions we've already raised about how useful those tests are.</p>




<p>Remember, external libraries are built to help people solve a general problem. When we design code for a project we are trying to solve specific problems. An abstraction that is handy for addressing the more general problem is not necessarily going to match the abstractions that help us.</p>




<h2>Well how do I test code that uses external types then?</h2>




<p>One way of looking at this is that we are switching from TDD-style tests with the aim of driving design to good old fashion "check this works" tests. In my experience the mindset is fairly different, as are the tests themselves.</p>




<p>One way of making this switch is to use the real types and do an integration test. Test drive our design down to a level of abstraction that is useful, then integration test over the boundary. Rather than checking our class interacts with an external type in a certain way, we check it gives us the behaviour we require.</p>




<p>If we are testing that we're using an ORM properly, we can use the real ORM and database and check that we can load up the required details after a save operation. If we're using <a href="http://www.castleproject.org/">Castle DynamicProxy</a>, then we can call our code that uses DynamicProxy and check the object we get back forwards to the interceptor we expect, rather than asserting we return a specific object from a mocked <code>ProxyGenerator</code> class we don't own and whose ins and outs we may not fully understand. We're checking the consequence of the action, rather than the details of the action itself.</p>




<p>This isn't to say that we add a whole host of tests around a component we don't own -- we really need to have a little faith in it working as advertised otherwise it would probably be quicker to just write our own. Instead the purpose of our test should focus on getting some degree of confidence that our use of the library has the basic desired behaviour. Rather than stating we need certain interactions with the library, we can use some simple, focussed cases with the real types to sanity check assumptions we made about the required interactions.</p>




<p>I've found that doing this frequently gives me much cleaner tests and implementations than if I had wrapped and tested the different elements of the interactions with types from another library. Provided the abstraction I've ended up with in my own code is OK, testing the behaviour tends not be too arduous.</p>




<p>As an alternative (or ideally complimentary) approach, we can use acceptance tests to make sure the entire feature works as expected (i.e. not just covering the call over the integration boundary, but exercising as much of the real feature as possible). Again, we're checking behaviour, not implementation specifics for types where we don't have a constant, transparent view into those implementations.</p>




<h2>What about when I just can't use the real type?</h2>




<p>There are times when using the real types is not practical. Maybe setting up an email server on each dev machine and checking the email account receives the required email is too much for our regular test suite (possibly a great test to run on a CI server or similar though). Or perhaps actually firing the doomsday device just isn't prudent for a test case that runs several times an hour... :)</p>




<p>In these cases we can create our own fake version of the external system or library. This is probably not going to come from our mocking framework of choice; as previously mentioned, once we find ourselves setting up complex interactions and behaviour using mocking framework constructs, we're probably doing it wrong. The important thing is to verify that our calls to the fake are compatible with the real classes, and where possible check that the behaviour matches what we expect. Martin Fowler calls these <a href="http://martinfowler.com/bliki/IntegrationContractTest.html">Integration Contract Tests</a>. The idea is that if we pass a certain range of values to our fake, the real version should also be able to handle these values. If we assume the real version behaves in a certain way and replicate that in our fake, we can write a test to try and verify that assumption.</p>




<p>In some cases we can use the real type with a different configuration as our fake. For example, we could run tests through our ORM configured to use an in-memory database. This won't test the final hop to our target RDBMS, but it will give us some confidence that our use of the API is correct, and we can setup more comprehensive tests against the real configuration on our CI server.</p>




<h2>Limitations of integration tests</h2>




<p>It would be remiss of me not to mention <a href="http://www.jbrains.ca">J.B. Rainsberger's</a> assertion that <a href="http://www.jbrains.ca/series/integrated-tests-are-a-scam">integration tests are a scam</a>, which is nicely <a href="http://b-speaking.blogspot.com/search/label/integration%20tests">summarised by Gabino Roche Jr</a>. I'm not proficient enough with this stuff to talk definitively on this, but I'll give you my 2 cents, then you should go through J.B.'s work and see how you can apply it. It's also worth looking at the <a href="http://www.growing-object-oriented-software.com/">GOOS book</a>; it has some really interesting examples of how to effectively use acceptance tests for driving development, including the need to deal with other libraries and systems.</p>




<p>My limited understanding of J.B.'s objections to integration tests is that they are not going to ensure correctness (you can't test exhaustively), they can be brittle and difficult to write and maintain, and that they can end up as a <a href="http://blog.thecodewhisperer.com/post/1325858548/integrated-tests-are-a-scam">"self-replicating virus"</a> when used as a crutch any time a programmer can't find a good, testable abstraction and therefore can't unit test the code.</p>




<p>Almost without fail whenever I have mocked a type I don't own I end up experiencing pain. Best case is I get lots of fairly useless abstractions that cover different operations within the external library's API. Worst case is I get lots of useless abstractions that don't work with the real API and are really difficult to change. This only leaves me with integration/acceptance-style tests for code that uses these types, so for me the question becomes one of how to write these tests effectively while trying to limit the legitimate problems J.B. has highlighted. I don't have the answer to this, but here's some guidelines I've been using:</p>




<ul>
<li>If possible, limit the amount of exposure we have to the external library. When <a href="https://github.com/nsubstitute/NSubstitute/blob/v1.0.0/Source/NSubstitute/Proxies/CastleDynamicProxy/CastleDynamicProxyFactory.cs">using DynamicProxy for NSubstitute</a> we could get away with using only a very small part of the library, which meant we <a href="https://github.com/nsubstitute/NSubstitute/blob/v1.0.0/Source/NSubstitute.Specs/Proxies/CastleDynamicProxy/CastleDynamicProxyFactorySpecs.cs">had only a few things to integration test</a>.</li>
<li>The abstractions we choose are important; they can determine how much work we have to do in each interaction with the external system. If there is a big mismatch between the abstraction we've driven out and the external code we need to work with, it might just be a sign we need a better fit for our requirements. (e.g. maybe we should persist using a document DB instead of an RDBMS? Or maybe just throwing data on the file system would fit better?)</li>
<li>Accept we can't test exhaustively, and focus on the most helpful cases. We generally want to assume the external stuff works as advertised, so we really just want to get some confidence that we are using it in a way that seems to give us the behaviour we expect.</li>
<li>If we keep coming up against integration bugs, adding integration tests is probably not the answer. We may need to examine ways we can push out more responsibility into unit-testable abstractions to simplify the integration.</li>
<li>Sometimes it is worth the time to develop a good, fake implementation of a complex external system. Examples include simulating external hardware, or using an in-memory database instead of an external one.</li>
<li>Make the tests easy to read and write. If you have long, complicated test setups we need to either look for better abstractions, or encapsulate the setup process in its own abstraction.</li>
</ul>




<h2>Conclusion</h2>




<p>In summary, my current thinking on the subject is that while using mocks and stubs is really beneficial when driving out the design of collaborators using TDD, it is important to identify when we need to stop test-driving and start testing. This is generally the point where we hit a type we don't own from a library or external system.</p>




<p>Using test doubles for types we don't own can end up with fragile tests that don't actually test much of value, or can even compromise our design and the effectiveness of the abstractions we use. Even when mocking a library we know really well we can end up with compromised abstractions and fragile tests due to relying on implementation details or assumptions based on previous versions of the library.</p>




<p>I've found switching to integration and acceptance tests to test over the boundary between my code and the other types a very useful alternative, but it is important to be aware that this approach can bring its own problems, and we need to try and mitigate these when writing our tests.</p>




<p>And finally, we should remember that this is just a guideline. If we find a situation where it is going to be both safe and simpler to mock a type we don't own, we may as well do so, but at least we've considered our options. :)</p>




<p>Have I got this all wrong? Please leave a comment or email me at davesquared.net. :)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why learning TDD is hard, and what to do about it]]></title>
    <link href="http://davesquared.net/2011/03/why-learning-tdd-is-hard-and-what-to-do.html"/>
    <updated>2011-03-21T17:42:00+11:00</updated>
    <id>http://davesquared.net/2011/03/why-learning-tdd-is-hard-and-what-to-do</id>
    <content type="html"><![CDATA[<p>I was recently asked to try some TDD coaching, which involved pairing with an insanely smart dev who was fairly new to TDD. One thing I found interesting was that many of the questions he asked were strikingly similar to the ones that tripped me up when I started out with TDD. Many of these seem to stem from incorrect assumptions about TDD, and the fact we both managed to get similar assumptions got me thinking about their source.</p>




<p>For this post we're going to take a look at why I feel TDD is so hard to learn, at least to the point where it can be used effectively for everyday coding tasks. From there we'll look at how and why TDD works, with the goal of giving new TDD practitioners a way to critically evaluate the introductory material they come across, and hopefully avoid some of the mistaken assumptions that can slow down the learning process. Finally we'll look at some ways this understanding can help people with some of the questions that commonly crop up starting TDD.</p>




<h2>Why is this so hard?</h2>




<p>I think there's a few reasons learning TDD is so difficult. Firstly, I think the hype around TDD can be quite detrimental to the learning process. Depending on which sources you read it is easy to get the impression that TDD is the only valid way of writing software, and that by following the simple steps your code will turn out gleaming with cleanliness, filled with only obvious, beautiful abstractions. In my experience, for all but the simplest of examples, this is just not true.</p>




<p>The unrealistic expectations set by the hype can cause people to become overly focused on the process, without understanding the rationale behind it. This can result in developers applying TDD in an ineffective manner. It can also become very frustrating for the learner, as they see the process that worked so well for implementing a <code>Stack</code> leave them lost and feeling inept when trying to use it to put something simple on a web page.</p>




<p>TDD is not a substitute for thinking. It is not a replacement for design skills. It is not actually driving anything -- the developer is, and so TDD will only go as well as its driver.  A more realistic view of TDD is that it is simply a tool that can be used to help you get a job done. Sure, it can be a remarkably useful and powerful tool, but in the end the goal is to get the job done*, not to start every bit of code with a failing test.</p>




<div class="note"><b>*</b> Writing shoddy code <a href="http://davesquared.net/2010/08/quality-vs-shipping.html">does not qualify as getting the job done</a>.</div>




<p>The other reason I believe TDD is hard to learn is that the TDD process itself actually gives you very little guidance as to how to practice it. When trying to learn something new a common first step is to learn a set of rules and go around applying them fairly mindlessly until enough experience is gained to know when to bend or break them. Eventually you start to see the rationale and patterns behind the rules and realise that how to apply them depends mainly on the context. Hence the infamous standard answer from experts: "it depends".</p>




<p>Now with TDD the <a href="http://davesquared.net/2008/02/tdd-is-easy.html">rules are trivially simple</a>. In fact they are so simple they don't give you enough guidance to mindlessly apply them until you gain enough proficiency to start achieving success with TDD. Instead you also need to know how to appropriately abstract your design, which requires in-depth knowledge of OO design, patterns, SOLID, DRY, etc. This can make TDD very hard to use effectively until some of these gaps are filled. Now TDD is an excellent way of highlighting deficiencies in these areas, but it isn't quite so forthcoming with the answers.</p>




<h2>Understand the mechanism, then learn to apply it</h2>




<p>I think one way to make TDD easier to learn is to make a concerted effort early on to understand how it works. Sure, start with the trivial, one class kind of example to learn the basics of the red-green-refactor process, like implementing a <code>Stack</code> [<a href="http://www.theserverside.net/tt/articles/content/TDD_Chapter/ch2.pdf">PDF link</a>] or <a href="http://osherove.com/tdd-kata-1/"><code>StringCalculator</code></a>, but then stop and think about what the process is doing.</p>




<p>We start by writing a failing test. Why? Our current tests are all passing, which gives us feedback that there are no known problems with it. By writing a test to expose a deficiency we are clarifying the problem we are trying to solve. Constraining the problem and therefore the possible solutions in this way makes it easier to solve (a divide and conquer-style approach). We're also making sure this code will be tested for both correctness and protection against regressions, but that's not exclusive to TDD; you can get that writing any decent automated tests.</p>




<p>Most importantly we're also sketching out a design, but rather than doing it on a whiteboard (which has its own advantages) we're doing it in code and getting immediate feedback as to how usable it is and how easy it is to test. Let's think about the kind of questions we need to think about in order to write this failing test:</p>




<ul>
<li>What is our System Under Test's (SUT) responsibility? i.e. What should it do, and when should it do it?</li>
<li>What is a convenient API for making the SUT do this?</li>
<li>What does the SUT need to discharge its responsibility? (Data? Collaborators?)</li>
<li>What output or side-effects are there to observe?</li>
<li>How can we tell it worked correctly? How is "correct" defined?</li>
</ul>




<p>These are all questions that should be asked at some point when writing any code. The power of TDD is that it provides us with a convenient tool for thinking about these questions. Rather than having to go through each question in turn, resolving potentially conflicting answers as we go, TDD lets us address the more abstract question of "How can I write the next test?", and in the process infer a solution that also addresses the more concrete questions.  This is why TDD is no better than its driver; you still need the skills and experience to answer the same questions. TDD merely gives you a tool to make it easier to think about them.</p>




<p>How exactly does this more abstract question help us? Like all useful abstractions, it gives us a way of thinking of a lot of details as a single whole. We can now tackle all those little details in a single train of thought, in a form that lets us apply both our technical skills and our creativity to finding a solution. And while tackling that single question we get the benefit of another of TDD's strengths: rapid, accurate feedback.</p>




<p>The process of writing the test gives us all sorts of feedback. The setup is too long or complicated? We've probably got too many collaborators (or are violating the Law of Demeter for them) and can try encapsulating them behind a new abstraction. Too many scenarios or test fixtures on the same SUT? Our SUT probably has too many responsibilities. Too hard to isolate the behaviour we're trying to test, or can't find how to write an assert for the behaviour? Maybe the current API or abstraction is wrong, and needs to be broken down differently? With TDD these problems become obvious very quickly. Solving them requires design skills, but so does any other approach to development. Writing the test first gives us ample opportunity to respond to this feedback, to sketch out our design, before committing to it. There is no cheaper time to change your code than before it is written.</p>




<p>Next we make our test pass with the simplest code we can justify. Funnily enough this is the least interesting part of the TDD process. We've done all the hard work in specifying our behaviour, now we just take the relatively simple step of making it work. Is it too hard to get working? Then we drop back to change our test; TDD has just given us feedback that our test is forcing too big a leap. Is the implementation so trivial it has obvious flaws? TDD has just given us feedback to tell us what our next test should expose.</p>




<p>Finally, we have the refactoring step. Our code works, but we've been focussed on passing a very specific test which only shows a very small picture of the application. Now is our chance to <a href="http://davesquared.net/2010/05/on-design-and-missing-forest-for-trees.html">zoom out and take in the entire application</a>. If we've  used a naive implementation to get our tests passing we can clean up this duplication, or extract methods to help our code be more self-describing. Even more important is <a href="http://davesquared.net/2010/05/on-design-and-missing-forest-for-trees.html">noticing larger scale duplication</a>; finding not only repeated code, but repeated activities that can be abstracted or made a transparent infrastructure concern.</p>




<h2>How does this help?</h2>




<p>Glad you asked! Provided you accept my rambling above, we now understand that TDD helps constrain the scope of the problem we're looking at, gives us a relatively simple way of thinking about the large number of design questions that apply to that problem, and provides rapid feedback on how well we are addressing these problems. This knowledge can help get us through some of the problems that can trip up people learning TDD.</p>




<h3>What test should I write?</h3>




<p>Well the flippant (albeit accurate) answer is whatever test provides you with the feedback you need to further your design. This answer is hardly going to help people just starting out unless we delve into a bit more detail.</p>




<p>What I struggled with most while learning TDD was moving from the simple, unit test driven examples to using TDD for real code. If I picked a unit test of a class lower down the hierarchy I'd have problems fitting the driven code back into the design. The problem was I wasn't thinking about how I could use my tests to give me feedback on what to do. If I had started with a test that described what I was trying to do then I would have been able to start driving out abstractions that actually helped me solve the problem, instead of blindly abstracting myself in circles. The trick is to pick a test that encapsulates what you know about the current problem.</p>




<p>Let's look at a more extreme case to illustrate this. Say we have a brand new project. In this case we're not even sure what classes we need. How can we use TDD to get feedback on our class design when we don't even have a class? Well, let's write a test that describes the feature (or story) we're working on. This will be an acceptance test rather than a unit test. <a href="http://davesquared.net/2010/06/tdd-not-utdd.html">No one said TDD has to exclusively use unit tests</a>, but as most examples for new practitioners concentrate on the process they all seem to use unit tests, hence the common misconception. This process will allow us to sketch out a basic architectural skeleton which we can flesh out and modify as we go along. Our test will provide us with feedback on how well we've managed to break the feature's requirements into manageable, testable abstractions. We'll also get a nice programmable interface to our system for future acceptance and integration tests. I found the <a href="http://www.growing-object-oriented-software.com/">Growing OO Software Guided By Tests books</a> a great source of examples of how to do this.</p>




<p>Once we have defined the basic behaviour for the feature via the acceptance test we can implement some of the infrastructure required for the test (build, CI etc.), and then pick the top class our test has discovered and start using more granular tests to drive out our classes. Or perhaps we won't even need more granular tests; the direction given by our acceptances tests might be sufficient for us to implement the feature.</p>




<h3>But you just wrote code before writing a test!</h3>




<p>Sure! TDD gives you feedback on design. When you already know what's required (say you're filling in pieces to comply with a set architecture) the design part of TDD is not giving you much benefit. The only real positive of writing a test first in this case is that you ensure the test fails first; it's a way of testing the test. This is test-first development, not TDD.</p>




<p>The less unknowns we have, the less our tests need to teach us, and the larger steps we can take.</p>




<h3>You picked an MVVM pattern and defined a view without a test!</h3>




<p>We've already learnt that TDD constrains the problem space we are looking at, as well as provides feedback on how well we are addressing this problem. Of course TDD isn't the only thing imposing constraints on us, nor our only source of feedback. Our choice of UI technology (WPF, an MVC framework, GTK, WebForms etc) imposes very real constraints on how we deal with our UI.</p>




<p>One way to address these constraints is to use a <a href="http://martinfowler.com/eaaDev/SeparatedPresentation.html">separated presentation pattern</a> such as <a href="http://davesquared.net/2010/02/attempt-at-simple-mvvm-with-wpf.html">MVVM</a>. In that case, we might define the properties and commands we need on a ViewModel without a single test. We're constrained by our UI technology, the UI pattern we've chosen, and the screen design we've worked out during discussions (another medium with rapid feedback) with our customers.</p>




<p>TDD isn't our only source of constraints and feedback. Where real constraints exist we don't need to force new ones by using TDD and ignoring reality.</p>




<div class="note"><b>Aside</b>:  I struggled with how to derive separated presentation patterns from first principles a lot when I first started: how could I use only tests to end up with MVP, MVVM etc? I think this becomes very difficult because at one end you have reality, the UI toolkit you need to work with, and at the other you just have your imagination and your tests. There are some hard constraints that simply don't pop out from tests, and they are just as relevant drivers as the feedback from the tests themselves. (That said, if anyone has derived a nice separated presentation implementation purely from TDD please share.)</div>




<h3>I'm stuck! I've test-driven myself into a hole and can't get out</h3>




<p>I've done this an embarrassing number of times. I've used tests to guide every line of code, <a href="http://davesquared.net/2010/05/test-driving-while-refactoring.html">including lines I write as part of refactoring</a>, and ended up stuck in a maze of pointless abstraction. Remember, TDD is not a substitute for thinking, <a href="http://davesquared.net/2010/02/improving-tdd-skills-via-treatment-of.html">you still need to be able to code</a>.</p>




<p>If TDD is not giving you the information you need, back up and try something else. Go through some ideas with colleagues as a white board. Use test coverage at a higher level of abstraction for feedback and work from there. Maybe spike some solutions (it is often faster to try out 3 different solutions than try and figure the best solution up front). TDD is one way of getting good constraints and feedback, but it is not the only way. Do what works for your situation, but make sure you think about it later and try and find the reason you got stuck.</p>




<div class="note"><b>Note:</b> If you can't get TDD to help you with a problem, make a note of it before trying something else, and come back to it later. It is important to see if it was just a problem TDD was ill-suited for, or whether a gap in your knowledge has been exposed. You'll never know the difference if you give up too easily on TDD. Pursuing these leads is what led me to discover mocking, IoC containers, conventions, BDD etc. And I'm far from finished finding gaps in my knowledge. ;)</div>




<h3>TDD? BDD? ATDD? Which should I use?</h3>




<p>You might notice that the use of TDD is very problem-specific (and developer-specific for that matter). Different problems raise different design issues that will require different types of feedback. This means we're going to have to apply TDD in different ways to give us the feedback we need to drive out our application's design. This can be top-down or bottom-up. In some cases we could lean very heavily on acceptance tests. Other times we might like to drive out implementation by a whole lot of unit tests. Still other problems may be well-served by end-to-end tests that actually hit an external database or service (these have their own issues, but if they are helping you get the feedback and design you need, then go for it!).</p>




<p>There isn't one right answer; you need to focus on getting the feedback that helps you get the job done.</p>




<h2>Conclusion</h2>




<p>While the rules for the TDD process are simple, learning to use it effectively is anything but. The rules themselves give us only minimal assistance when we're starting out, and the hype surrounding TDD can give us unrealistic expectations about what it will do for us.</p>




<p>We can address this by trying to understand how TDD works; the rationale behind the rules. We've seen that TDD is really just a way of constraining a problem, encapsulating the process of design by using the abstraction of a test, and providing rapid feedback as to how well our design is going. The process essentially gives us a nice way to think about and iteratively do design.</p>




<p>Once we understand this we can start trying to apply TDD in a more targeted manner, deliberately applying it in ways to give us the feedback we need to make design decisions. We won't restrict ourselves to unit tests alone, but will switch between testing at different levels of abstraction without feeling guilt over not using an unit test as all the introductory examples seem to. Nor will we feel guilty when we make a decision without TDD's feedback, instead relying on other feedback like other systems we're interacting with, the UI technology we're using, or the persistence technology that's been mandated company-wide.</p>




<p>We're also not going to feel inept when we fail to apply the deceptively simple TDD rules, as we realise TDD is just a tool to help us think about design problems, and that there are always going to be gaps in our knowledge and experience we need to fill before we can solve new problems. The fact TDD has illustrated these gaps quickly gives us an opportunity to learn right away, rather than leaving us in blissful ignorance until things explode as a deadline looms, or worse, keeping us unaware of and repeating the same mistakes time after time.</p>




<p>Design is hard. TDD gives us one great way of thinking about it and learning how to do it better.</p>




<p>If you're currently learning TDD I hope these ramblings have given you a different way of thinking about it that will help make your journey a little easier. Best of luck -- I think you'll find TDD well worth the effort! :)</p>




<h2>In case this post wasn't long enough for you...</h2>




<p>This post summarises what I've learned to date from my TDD journey. Have a look at the following posts if you'd to read how my thoughts on this have evolved over the years (they're listed from oldest to latest). They'll also provide some more details and/or examples of some of the ideas discussed in this post.</p>




<ul>
<li><a href="http://davesquared.net/2009/06/moving-to-scenario-based-unit-testing.html">Scenario-based testing</a></li>
<li><a href="http://davesquared.net/2009/09/example-of-driving-design-through-top.html">Example of driving design through top-down testing</a></li>
<li>Calculators and a tale of two TDD styles <a href="http://davesquared.net/2009/10/calculators-and-tale-of-two-tdds-pt-1.html">Part 1</a>, <a href="http://davesquared.net/2009/10/calculators-and-tale-of-two-tdds-pt-2.html">Part 2</a>, <a href="http://davesquared.net/2009/10/calculators-and-tale-of-two-tdds-pt-3.html">Part 3</a></li>
<li><a href="http://davesquared.net/2009/11/favour-test-driving-logic-over-data.html">Favour test driving logic over data</a></li>
<li><a href="http://davesquared.net/2010/02/what-exactly-is-tdd-driving.html">What exactly is TDD driving?</a></li>
<li><a href="http://davesquared.net/2010/02/improving-tdd-skills-via-treatment-of.html">Improving TDD skills via treatment of test infection</a></li>
<li><a href="http://davesquared.net/2010/05/test-driving-while-refactoring.html">Don't test drive while refactoring</a></li>
<li><a href="http://davesquared.net/2010/06/tdd-not-utdd.html">TDD vs UTDD</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TDD, not UTDD]]></title>
    <link href="http://davesquared.net/2010/06/tdd-not-utdd.html"/>
    <updated>2010-06-04T23:16:00+10:00</updated>
    <id>http://davesquared.net/2010/06/tdd-not-utdd</id>
    <content type="html"><![CDATA[<p>When I first learned TDD I was taught that the first step in the process is to write a failing <i>test</i>, not specifically a failing <i>unit test</i>. After all, it is Test Driven Development, not Unit Test Driven Development. I even read books that were apparently written using a TDD-style approach, without a unit test in sight*. This didn't stop me from focussing almost entirely on using unit tests for TDD.</p>




<p>Fast forward a few years and I'm now finding a lots of benefits in other forms of testing for TDD, to complement the <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=126923">traditional unit test</a>.</p>




<div style="font-size: x-small; font-style: italic;"><b>*</b> In case you're wondering how this works:  write the goal for an unwritten section, write the section, verify it meets the goal, edit. Red, green, refactor. :)</div>




<h2>Accepting acceptance tests</h2>




<p>Acceptance testing is a practice that seems very easy to get wrong in ways that cause a lot of friction, resulting in it being ignored or given only cursory treatment by developers. This is unfortunate, as I see acceptance testing as essential for effective TDD.</p>




<p>In Steve Freeman's and Nat Pryce's <a href="http://www.amazon.com/Growing-Object-Oriented-Software-Guided-Tests/dp/0321503627">Growing Object Oriented Software, Guided by Tests</a>, the first failing test they write is an acceptance test for the feature they are working on. They then drill down into unit tests so they can take small steps to incrementally build an implementation that passes the acceptance criteria. This was how I was initially introduced to TDD from reading about Extreme Programming (XP): an outer TDD cycle with acceptance tests, and an inner cycle with unit tests that had several iterations to get the acceptance test to pass.</p>




<p>The key to effective acceptance tests (at least for me, YMMV) is making sure they exercise a specific feature of the system from top to bottom, using as much of the real system as possible. It should clearly specify the behaviour of that feature -- once it passes you should be fairly confident that the customer's requirements for that feature have been met.</p>




<p>The main benefit I've found from acceptance testing is that the feedback from these tests help produce an architecture that is flexible, maintainable, and scriptable by virtue of being testable at such a high level. They also help me focus on exactly what I need to get this feature done, which in turn helps guide where I should start applying unit tests to drive the more specific elements of my design.</p>


<p></p>

<p>These benefits, using tests to define and design, are fairly universal to TDD regardless of which type of tests are used. In the case of acceptance tests, the large scope of the tests provide feedback on the larger aspects of the design.</p>




<p>I've also found acceptance tests to be invaluable when I've had to make radical design changes (e.g. when I've stuffed up somewhere), letting me cull over-specified unit tests and make sweeping changes while still having enough coverage to be confident the software works.</p>




<p>If you're doing TDD but not using acceptance tests, or have tried acceptance testing before but haven't been able to make it work for you, I'd really recommend giving it another shot. Don't worry about them being customer-writable (or even customer-readable for now, provided you can explain what is being tested), don't worry about what tool you use, just get them working. You're architecture will thank you for it. :)</p>


<p></p>

<h2>Don't mock types you don't own -- integration test them!</h2>




<p>Recently I was test driving some code that uses Castle DynamicProxy. I mocked out the Castle interface and checked my subject under test interacted with that library in a way that I thought was correct. The problem here is I do not own the Castle type, and <a href="http://stevef.truemesh.com/archives/000194.html">you should not mock types you don't own</a>.</p>




<p>Mocking types you don't own gives you very little in the way of ensuring correctness, and is potentially misleading in terms of the design guidance it provides. The problem is that you are testing based on your assumption of how the type works, not how it actually works. Sure, you're testing that your code correctly calls the method you told it to, but what about testing it calls the correct method? If the type changes in a later version, or if it's behaviour is slightly different than you expect under different conditions or arguments, then your tests can pass but your software fails. A misleading test like this can be more harmful than having no test.</p>




<div class="note"><b>Aside:</b> The same criticism can be levelled at mocking in general. The difference is that you have tests defining how your own types work, and have the ability to easily change the types if they do not function as required.</div>




<p>Another drawback, especially if you are working with libraries or frameworks that are not designed in a particularly test-friendly way (to put it diplomatically**), is that you may end up starting to push the behaviour of those libraries into those mocks in order for your class under test to interact with them in a meaningful way. Once you start simulating behaviour in your mocks you are doing it wrong -- you are well on your way to brittle, over-specified, complicated tests.</p>




<div style="font-size: x-small; font-style: italic"><b>**</b>  I'm not talking about Castle here, it's awesome. I will remind readers I have worked with SharePoint before... :)</div>




<p>Of course, if you are avoiding mocking types you don't own, this implies you need to use the real types, which means we are in the realms of integration testing. For my Castle-calling code, I ended up unit testing down to my own class that needed to use Castle to achieve something, then writing integration tests with real Castle objects to ensure that my class actually did use Castle correctly. This ended up being much more valuable to me, and much more flexible. It was more valuable because my tests actually told me my class was using the library correctly and was getting the results my system required, rather than just calling the method I thought was needed. It was more flexible because I had not over-specified every interaction with the third-party library, and so could easily and independently vary both my code and how my code interacted with that library.</p>




<p>I've had a habit of avoiding integration tests as I always assumed they had too wider scope and were too slow to be useful. Now I look forward to hitting a case I can easily cover with integration tests, as it means I've reached the bottom of my software's abstractions and can just test a concrete piece that actually does some real work by interacting with its environment.</p>




<div class="note"><b>Note:</b> A few words of caution about integration tests. I wouldn't recommend switching to integration tests until you are at the very bottom layers of abstraction. Test-drive down until you reach a single class that is an effective facade for some behaviour from a third-party library, then use integration tests to make sure it works as required. Integration tests can also slow down your test suite if they end up hitting the file system, database etc., in which case you should make sure you are able to run them separately from your unit tests and only run them when needed (such as prior to checkin or when you change something related).</div>




<h2>Conclusion</h2>




<p>I still rely very heavily on unit tests when test driving software, but I feel it is really important to know when to use other forms of testing with TDD (and without TDD for that matter). Acceptance tests are a great way to kick off a TDD cycle from the top down, while integration tests are invaluable once you reach the bottom and need to write the code that interacts with the rest of the world. Then there's unit testing for everything in between.</p>




<p>Finally, of course, there's manual, exploratory testing. This probably won't feature too much in your standard TDD cycle, but is so important for checking your software actually works that it didn't feel right not to mention it. :)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Friends don't let friends test-drive while refactoring]]></title>
    <link href="http://davesquared.net/2010/05/test-driving-while-refactoring.html"/>
    <updated>2010-05-06T22:25:00+10:00</updated>
    <id>http://davesquared.net/2010/05/test-driving-while-refactoring</id>
    <content type="html"><![CDATA[<p>I've generally tended to think about refactoring purely in the context of Test Driven
Development. You write a failing test, make it pass, then refactor to
tidy up. The main stimulus for the refactoring has been feedback from
the tests.</p>




<p>It was only very recently I realised that I should probably think about refactoring separately from TDD. Although it is obviously an essential part of the TDD process, it is also a powerful technique in its own right. As I discovered, treating them interchangeably can lead to trouble.</p>




<h2>Refactoring into a test-driven hole</h2>




<p>I had a class that had grown too big. I wasn't sure of the right design to split up this class, so my
first approach was to fall back on writing tests. I adjusted the tests for the class in question and drove out some new collaborators. This broke a whole lot of my acceptance tests, but I figured that was ok because my unit tests were passing. I continued driving myself deeper and deeper into
a mess. My unit tests were giving me feedback as to the design I was
driving out, but because I had lost the feedback of my acceptance
tests I had no idea how far I had strayed from a correct, functioning
implementation. I knew that I had lots of broken tests, and no matter
how much I drove out the green bar remained painfully elusive.</p>




<p>A few hours later I took stock. The design direction seemed to be
workable although unreasonably complex. I was fairly confident it would only take a little longer to polish off the final pieces, and I had reduced the failing
acceptance tests from 30 to about 19, but the whole thing just didn't
seem right. For one, I hadn't been able to check in with tests failing
so this was going to be a huge change. For another, I'd been driving
out all this testable design without the strong link the
acceptance tests provide to the actual result I was trying to achieve.</p>




<p>I had pretty much turned from a developer into an abstraction factory. Time to revert. (Actually, <code>git checkout -b garbage</code> and commit to the new branch, just in case... :))</p>




<h2>Refactoring without thinking about TDD</h2>




<p>The second time I tried this I was determined to take little steps. I
had no unit tests to guide these steps, just the <a href="http://davesquared.net/2009/01/introduction-to-solid-principles-of-oo.html">SOLID principles</a> and the feedback from my existing test suite, which mainly showed my class was too big and
was going to grow bigger with future changes (apparently gravity applies to code,
too :)).</p>




<p>So I started extracting small bits of behaviour and pushing them down
into collaborators (which my existing class new'ed up, violating DI but keeping my code functional). My unit tests now became integration tests (or at least covered a bigger unit), but most importantly I still had my acceptance tests telling me the software still worked. I found it very
useful to keep testability in mind for all new classes I was creating,
but the design I was refactoring toward was just based on isolating
some of the responsibilities my class had accrued.</p>




<p>Once I had pushed down the messiest stuff into a few new classes at varying levels
of abstraction, being careful to keep the tests green all the time, I went and back-filled some unit tests for the original class and its interaction with its new collaborators (replacing the poor man's DI with real DI). This left the new stuff uncovered by unit tests, but still safe due to the integration tests. I could then go back to my standard TDD approach for the final push from the new class down to a finished implementation.</p>




<p>The style of TDD I use helps me to decompose problems and abstract and encapsulate data and behaviour in a way that appeals to my limited sense of aesthetics. This meant that before I started the refactoring I was relatively happy with how the design broke the problem down. Switching to purely refactoring mode meant I could keep the same basic problem decomposition and just tinker with the implementation. I didn't really need TDD to drive the change; it was more a case of rephrasing the existing problem breakdown.</p>




<h2>Lessons learned</h2>




<p>I (re-)learned a few important lessons from this experience. The first was that small steps are absolutely essential whenever you're not 100% confident with what you're doing. Whenever you've taken a big step away from the green bar, it's time to revert and try smaller steps. (A big step from one green to another is fine if you can manage it.)</p>




<p>The second is that I really need to separate my test-driving from refactoring. I'm either wearing my test-driven design hat, or my refactoring hat. Once I've passed my failing test and can see the green test runner bar of happiness, then I really need to stop thinking about tests and start driving using the refactoring process: small, non-breaking, non-behaviour-altering changes.</p>




<p>Come to think of it, refactoring probably shouldn't involve changing tests at all (unless I've made the mistake of having my tests overly-specify an implementation). As refactoring doesn't change behaviour, and our tests are covering the behaviour or result of that behaviour, then our tests shouldn't need to change. Once we switch to altering tests as a part of a big change we're really in the realms of <a href="http://davesquared.net/2010/02/refactor-or-redesign.html">redesign rather than refactoring</a>. Of course, once we've finished refactoring and our tests are still green, we then have some confidence our code is correct and can resume the test-driven cycle and change our tests however we like.</p>




<p>When I asked a question about this on Twitter, <a href="http://twitter.com/JakCharlton/status/13104500312">Jak Charlton summarised it perfectly</a>: "You can change tests or implementations, but not both at the same time". (That's right, it takes me 1,000 words to say what most people can say with 140 characters. I'd appreciate any verbosity-cures donated to the comment box below. Thanks. :))</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Improving TDD skills via treatment of test infection]]></title>
    <link href="http://davesquared.net/2010/02/improving-tdd-skills-via-treatment-of.html"/>
    <updated>2010-02-24T23:22:00+11:00</updated>
    <id>http://davesquared.net/2010/02/improving-tdd-skills-via-treatment-of</id>
    <content type="html"><![CDATA[<p>TDD is a technique that can be used as an effective design tool. However it is quite easy (or was for me) to become "<a href="http://c2.com/cgi/wiki/wiki?TestInfected">test infected</a>". Normally this term is used as a positive description of someone who has seen how good TDD is and doesn't like to work without it, but in some cases this infection can turn nasty.</p>




<p>Symptoms of serious infection include becoming incapable of working without tests, an insatiable hunger for reference material on TDD showing the way to do it right (a worrying delusion, as there is no silver bullet), and a severe dependency on the process to the point where the patient sometimes gets blocked waiting for the tests to give them an answer to a question that they really just need to nut out for themselves.</p>




<p>As I mentioned in my last post on <a href="http://davesquared.net/2010/02/what-exactly-is-tdd-driving.html">driving different aspects of code using TDD</a>, it is really the developer using the tests to drive code, rather than the process itself driving the developer to write great code. Unfortunately severe infections can result in the developer subconciously believing the latter. Luckily the prognosis for these cases is quite positive, the infection responding quite well to simple remedial action. In many cases the patient can recover to be even more effective than they were prior to infection.</p>




<p>If you believe you may be suffering from a severe case of test-infection, try this experiment: force youself to write some code without using tests. High level acceptance tests are ok, just not TDD or unit tests. Something like the <a href="http://codekata.pragprog.com/2007/01/kata_two_karate.html">Karate Chop kata</a> is ideal, because it is something that is normally very nice to drive with classical TDD tests.</p>




<p>When I tried this and a few similar exercises I initially found them suprisingly difficult without TDD. These were basic problems I would have solved beautifully prior to learning TDD, and here I was struggling on the same, simple exercises, all because I couldn't lean on my favourite process. I had developed quite a severe case. I've also seen this experiment trip up other, far more intelligent people in the same way.</p>




<p>I had become a slave to the process to the point where I had surrended some of the critical thinking I used to use before picking up TDD. I actually had to retrain myself to <a href="http://weblogs.asp.net/fbouma/archive/2009/07/26/think-first-doing-is-for-later.aspx">think first</a> by repeating a few of these simple kata-like exercises until I was able to code again without the aid of TDD. I could then use my newly-rediscovered ability to think to target how I applied TDD (or other techniques) to problems. I'm confident this process can work for other patients as well. This is not to say patients need to enjoy working without the guidance of tests, but they should be able to code and design by thinking through the problem instead of relying on TDD. TDD is a tool, not the goal.</p>




<p>Once a developer gets to the point where they can write decent code both with and without TDD, they are now in a great position to wield TDD really effectively as a design tool. Rather than being driven by tests, or by designing upfront or on-the-fly without the feedback given by TDD, the developer can start writing tests that push both the design and implementation in the direction he or she feels it should go. The tests aren't driving: they are a means to an end, a stepping stone the developer uses to push the design to a point where the current feature can be implemented safely, quickly and cleanly. I think this level is where true mastery of TDD is achieved.</p>




<p>One day I hope to get there, but for now I'm happy to keep honing my fairly blunt use of TDD in the knowledge I'm using it to help me flounder along in the basic direction I want to go, rather than as a crutch.</p>




<p><i>Recovered from your own bout of test infection to become a better TDDer? I'd love to hear about it -- please leave a comment! :)</i></p>

]]></content>
  </entry>
  
</feed>
